<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>DreamerV3 on Craftax – Agent Performance Analysis</title>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,700;1,8..60,400&family=DM+Sans:wght@400;500;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg: #0d1117;
  --surface: #161b22;
  --border: #30363d;
  --text: #c9d1d9;
  --muted: #8b949e;
  --accent: #58a6ff;
  --green: #3fb950;
  --red: #f85149;
  --orange: #d29922;
  --purple: #bc8cff;
  --cyan: #39d2c0;
  --pink: #f778ba;
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: 'DM Sans', sans-serif;
  line-height: 1.7;
  padding: 0;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
  padding: 40px 24px;
}

/* Hero */
.hero {
  text-align: center;
  padding: 80px 24px 60px;
  background: linear-gradient(180deg, #0d1117 0%, #161b22 100%);
  border-bottom: 1px solid var(--border);
}

.hero h1 {
  font-family: 'Source Serif 4', serif;
  font-size: 2.8rem;
  font-weight: 700;
  background: linear-gradient(135deg, var(--accent), var(--cyan));
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  margin-bottom: 12px;
}

.hero .subtitle {
  font-size: 1.15rem;
  color: var(--muted);
  max-width: 700px;
  margin: 0 auto;
}

.hero .meta {
  margin-top: 20px;
  display: flex;
  justify-content: center;
  gap: 32px;
  flex-wrap: wrap;
}

.hero .meta span {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.85rem;
  color: var(--muted);
  padding: 6px 14px;
  background: var(--bg);
  border: 1px solid var(--border);
  border-radius: 6px;
}

/* KPI Cards */
.kpi-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 16px;
  margin: 40px 0;
}

.kpi-card {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 24px;
  text-align: center;
}

.kpi-card .value {
  font-family: 'JetBrains Mono', monospace;
  font-size: 2rem;
  font-weight: 600;
  margin-bottom: 4px;
}

.kpi-card .label {
  font-size: 0.85rem;
  color: var(--muted);
  text-transform: uppercase;
  letter-spacing: 0.05em;
}

/* Section */
.section {
  margin: 60px 0;
}

.section-header {
  display: flex;
  align-items: center;
  gap: 12px;
  margin-bottom: 8px;
}

.section-header .number {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.85rem;
  color: var(--accent);
  background: rgba(88,166,255,0.1);
  padding: 4px 10px;
  border-radius: 6px;
  border: 1px solid rgba(88,166,255,0.2);
}

.section h2 {
  font-family: 'Source Serif 4', serif;
  font-size: 1.7rem;
  font-weight: 700;
  color: var(--text);
}

.section .description {
  color: var(--muted);
  margin-bottom: 24px;
  font-size: 0.95rem;
}

/* Figure */
.figure-block {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 12px;
  overflow: hidden;
  margin: 24px 0;
}

.figure-block img {
  width: 100%;
  display: block;
}

.figure-caption {
  padding: 16px 20px;
  font-size: 0.85rem;
  color: var(--muted);
  border-top: 1px solid var(--border);
}

/* Analysis text */
.analysis {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 28px 32px;
  margin: 20px 0;
  line-height: 1.85;
}

.analysis p {
  margin-bottom: 16px;
}

.analysis p:last-child { margin-bottom: 0; }

.analysis strong { color: var(--text); }

.analysis .metric {
  font-family: 'JetBrains Mono', monospace;
  color: var(--accent);
  font-size: 0.92em;
}

.analysis .good { color: var(--green); }
.analysis .warn { color: var(--orange); }
.analysis .bad { color: var(--red); }

/* Inline stat highlight */
.stat-highlight {
  display: inline-flex;
  align-items: baseline;
  gap: 4px;
  padding: 2px 8px;
  background: rgba(88,166,255,0.08);
  border-radius: 4px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.9em;
}

/* Achievement table */
.ach-table {
  width: 100%;
  border-collapse: collapse;
  font-size: 0.9rem;
  margin: 16px 0;
}

.ach-table th {
  text-align: left;
  padding: 10px 14px;
  border-bottom: 2px solid var(--border);
  color: var(--muted);
  font-size: 0.8rem;
  text-transform: uppercase;
  letter-spacing: 0.04em;
}

.ach-table td {
  padding: 8px 14px;
  border-bottom: 1px solid var(--border);
}

.ach-table tr:last-child td { border-bottom: none; }

.ach-table .tier-badge {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.75rem;
  padding: 2px 8px;
  border-radius: 4px;
  display: inline-block;
}

.tier-0 { background: rgba(63,185,80,0.15); color: var(--green); }
.tier-1 { background: rgba(88,166,255,0.15); color: var(--accent); }
.tier-2 { background: rgba(210,153,34,0.15); color: var(--orange); }
.tier-3 { background: rgba(188,140,255,0.15); color: var(--purple); }
.tier-4 { background: rgba(248,81,73,0.15); color: var(--red); }

.rate-bar {
  display: flex;
  align-items: center;
  gap: 8px;
}

.rate-bar .bar {
  height: 8px;
  border-radius: 4px;
  min-width: 2px;
}

.rate-bar .val {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.85rem;
  min-width: 48px;
}

/* Config badge */
.config-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
  gap: 12px;
  margin: 16px 0;
}

.config-item {
  display: flex;
  justify-content: space-between;
  padding: 10px 16px;
  background: var(--bg);
  border: 1px solid var(--border);
  border-radius: 8px;
  font-size: 0.88rem;
}

.config-item .key { color: var(--muted); }
.config-item .val {
  font-family: 'JetBrains Mono', monospace;
  color: var(--accent);
}

/* Callout */
.callout {
  padding: 20px 24px;
  border-radius: 10px;
  margin: 20px 0;
  border-left: 4px solid;
}

.callout.info {
  background: rgba(88,166,255,0.06);
  border-color: var(--accent);
}

.callout.success {
  background: rgba(63,185,80,0.06);
  border-color: var(--green);
}

.callout.warning {
  background: rgba(210,153,34,0.06);
  border-color: var(--orange);
}

.callout.critical {
  background: rgba(248,81,73,0.06);
  border-color: var(--red);
}

.callout .callout-title {
  font-weight: 700;
  margin-bottom: 6px;
  font-size: 0.95rem;
}

.callout p { font-size: 0.92rem; margin: 0; }

/* Footer */
footer {
  text-align: center;
  padding: 40px;
  border-top: 1px solid var(--border);
  color: var(--muted);
  font-size: 0.85rem;
}
</style>
</head>
<body>

<!-- HERO -->
<div class="hero">
  <h1>DreamerV3 × Craftax</h1>
  <p class="subtitle">Comprehensive Performance Analysis — Single-task continual learning with a DreamerV3 world model on Craftax (500k environment steps)</p>
  <div class="meta">
    <span>Backbone: DreamerV3 (RSSM)</span>
    <span>Steps: 500k</span>
    <span>Episodes: 1,768</span>
    <span>Seed: 42</span>
    <span>GPU: RTX 4090</span>
  </div>
</div>

<div class="container">

<!-- KPIs -->
<div class="kpi-grid">
  <div class="kpi-card">
    <div class="value" style="color:var(--green)">3.78</div>
    <div class="label">Mean Return (final)</div>
  </div>
  <div class="kpi-card">
    <div class="value" style="color:var(--accent)">7.1</div>
    <div class="label">Max Return</div>
  </div>
  <div class="kpi-card">
    <div class="value" style="color:var(--cyan)">100%</div>
    <div class="label">Success Rate</div>
  </div>
  <div class="kpi-card">
    <div class="value" style="color:var(--orange)">9 / 22</div>
    <div class="label">Achievements Discovered</div>
  </div>
  <div class="kpi-card">
    <div class="value" style="color:var(--purple)">3.6×</div>
    <div class="label">Improvement Ratio</div>
  </div>
  <div class="kpi-card">
    <div class="value" style="color:var(--red)">0.030</div>
    <div class="label">Aggregate Forgetting</div>
  </div>
</div>


<!-- ═══════════ SECTION 1: Episode Return & Length ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">01</span>
    <h2>Episode Return & Length</h2>
  </div>
  <p class="description">Raw episode-level return and episode length, showing per-episode scores and the smoothed trend.</p>

  <div class="figure-block">
    <img src="fig1_return_length.png" alt="Episode Return and Length">
    <div class="figure-caption">Figure 1: Episode return (blue, left axis) and episode length (orange, right axis) over 500k training steps. Faint traces show individual episodes; bold lines are exponential moving averages (EMA-80).</div>
  </div>

  <div class="analysis">
    <p><strong>Episode Return</strong> measures the total reward the agent accumulates during a single episode in the Craftax environment. Each "achievement" (e.g., collecting wood, crafting a tool, defeating an enemy) contributes a discrete +0.1 reward, so the score directly reflects the number of achievements the agent manages to unlock per episode. A return of <span class="metric">3.78</span> means the agent unlocks roughly 3–4 unique achievements per life on average by the end of training.</p>

    <p>The training curve shows three distinct phases. During the <strong>initial ramp-up</strong> (0–100k steps), the agent starts with near-zero return and rapidly climbs as the world model learns the basic visual dynamics and reward structure of Craftax, reaching a mean of <span class="stat-highlight">1.56</span>. In the <strong>steep learning phase</strong> (100k–250k), performance nearly triples as the policy discovers higher-reward strategies and chains of achievements, jumping from ~1.5 to ~3.5. The <strong>plateau phase</strong> (250k–500k) sees diminishing marginal gains — the agent oscillates between <span class="stat-highlight">3.5–4.1</span>, suggesting the policy has converged on a stable behavioral repertoire within the current model capacity and exploration budget.</p>

    <p><strong>Episode Length</strong> measures how many environment time-steps the agent survives per episode (Craftax episodes terminate on death or after a maximum horizon). Episode length increases from ~180 steps early in training to ~320 steps by the end, a <span class="metric good">78% increase</span> in survival time. This is significant: longer episodes mean the agent learns survival-critical behaviors (eating, drinking, avoiding dangers) which provide more time to pursue higher-tier achievements. The positive correlation between length and return confirms the agent is getting better at <em>staying alive while being productive</em>, rather than just getting lucky with a few early achievements before dying.</p>

    <p>The variance in both metrics remains substantial throughout training (note the faint scatter), which is characteristic of Craftax's procedurally-generated nature — each episode presents a different map layout, resource distribution, and enemy placement, making some episodes inherently harder than others.</p>
  </div>
</div>


<!-- ═══════════ SECTION 2: Rolling Mean & Success Rate ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">02</span>
    <h2>Rolling Mean Return & Success Rate</h2>
  </div>
  <p class="description">100-episode sliding window metrics tracking aggregate performance stability.</p>

  <div class="figure-block">
    <img src="fig2_mean_return_success.png" alt="Mean Return and Success Rate">
    <div class="figure-caption">Figure 2: Left — rolling 100-episode mean return converging at 3.78. Right — rolling success rate (episodes scoring ≥ 1.0) stabilizing at 100%.</div>
  </div>

  <div class="analysis">
    <p><strong>Rolling Mean Return</strong> computes the average return over the last 100 episodes, giving a noise-filtered view of performance trend. This metric is the standard evaluation protocol in Craftax benchmarks and allows comparison across methods. The curve confirms monotonic improvement with no regression periods, settling at a final value of <span class="metric good">3.78</span>. For context, the original Crafter benchmark (which Craftax reimplements in JAX) reports DreamerV3 achieving approximately 14.5 mean score at 1M steps — but that's on the standard Crafter which has a different scoring scheme. At 500k steps with the achievement-based reward, a score of 3.78 is reasonable for mid-training.</p>

    <p><strong>Success Rate</strong> is defined here with a threshold of 1.0 — any episode achieving at least 1.0 total return counts as a "success." The agent reaches <span class="metric good">100% success rate</span> by around 150k steps, meaning every single episode in the final window unlocks at least one achievement. This is a clean binary indicator that the agent has learned the most basic survival and resource-gathering behaviors reliably. However, this metric saturates early and doesn't discriminate between an episode scoring 1.0 and one scoring 7.0, so it's more useful as a "sanity check" than a performance discriminator at this stage.</p>

    <p>The rapid convergence of success rate to 100% (within the first third of training) while mean return continues climbing confirms that the agent's learning bottleneck has shifted from "can it achieve anything?" to "can it chain multiple achievements in a single life?" — exactly the transition we'd expect from a working world model that has internalized the basic state dynamics.</p>
  </div>
</div>


<!-- ═══════════ SECTION 3: Achievement Heatmap ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">03</span>
    <h2>Achievement Success Rates over Training</h2>
  </div>
  <p class="description">Temporal heatmap of all 22 Craftax achievements — rows are achievements, columns are training progress, color intensity is 100-episode rolling success rate.</p>

  <div class="figure-block">
    <img src="fig3_achievement_heatmap.png" alt="Achievement Heatmap">
    <div class="figure-caption">Figure 3: Achievement success rate heatmap. White horizontal lines separate tiers (0–4). Color scale: black = 0%, bright yellow = 100%. Y-axis labels are colored by tier.</div>
  </div>

  <div class="analysis">
    <p>This heatmap is the most information-dense visualization in the report, encoding the temporal evolution of all 22 achievement success rates simultaneously. Each row corresponds to one Craftax achievement, each column to a time bin, and the color intensity represents the 100-episode rolling success rate at that point in training.</p>

    <p>The <strong>clearly visible bright bands</strong> correspond to achievements the agent has mastered: <span class="metric good">collect_sapling</span> (100%), <span class="metric good">place_stone</span> (100%), <span class="metric">make_iron_sword</span> (97%), and <span class="metric">collect_wood</span> (88%). These light up early and remain consistently bright, indicating stable mastery with minimal forgetting. The <span class="metric">collect_drink</span> achievement (56%) shows a dimmer but sustained band, indicating the agent has learned to drink water in a majority of episodes but not universally.</p>

    <p>The <strong>dark horizontal bands</strong> across the entire training span — covering most of Tiers 2, 3, and 4 — reveal that the agent never discovers a viable policy for these achievements. The complete darkness of <span class="metric bad">place_furnace</span>, <span class="metric bad">collect_coal</span>, <span class="metric bad">make_stone_pickaxe</span>, <span class="metric bad">make_stone_sword</span>, and all diamond/combat achievements indicates a hard exploration barrier. The agent has not found rewarding trajectories that involve these multi-step crafting sequences within 500k steps.</p>

    <p><strong>Tier separation is clearly visible</strong> — there's a stark brightness drop at the Tier 0/1 boundary and again at Tier 1/2. This aligns with the hierarchical prerequisite structure of Craftax: higher-tier achievements require completing sequences of lower-tier ones, and the agent hasn't fully mastered Tier 1 prerequisites yet.</p>
  </div>
</div>


<!-- ═══════════ SECTION 4: Final Achievement Bar Chart ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">04</span>
    <h2>Final Achievement Rates by Tier</h2>
  </div>
  <p class="description">Per-achievement success rate computed over the final 100 episodes, grouped and colored by tech-tree tier.</p>

  <div class="figure-block">
    <img src="fig4_final_achievements.png" alt="Final Achievement Rates">
    <div class="figure-caption">Figure 4: Horizontal bar chart of final achievement rates. Colors: green = Tier 0, blue = Tier 1, orange = Tier 2, purple = Tier 3, red = Tier 4.</div>
  </div>

  <div class="analysis">
    <p>This chart provides the definitive snapshot of what the agent can and cannot do at the end of training. Let's walk through each achievement:</p>

    <p><strong>Tier 0 – Basic (avg 37.7%):</strong> This tier captures fundamental resource gathering and survival. <span class="metric good">collect_sapling</span> at 100% and <span class="metric good">collect_wood</span> at 88% show the agent reliably gathers basic plant resources. <span class="metric">collect_drink</span> at 56% means the agent finds water in just over half of episodes — a survival-critical behavior since dehydration kills. <span class="metric warn">eat_cow</span> at 11% and <span class="metric warn">place_table</span> at 9% are surprisingly low; the agent hasn't learned to reliably hunt or craft a basic workbench. <span class="metric bad">make_wood_pickaxe</span> and <span class="metric bad">make_wood_sword</span> at 0% are concerning — these require place_table as a prerequisite, so the 9% table rate bottlenecks all downstream tool crafting.</p>

    <p><strong>Tier 1 – Stone (avg 14.7%):</strong> <span class="metric good">place_stone</span> at 100% is anomalous and warrants investigation — this should require stone collection, yet <span class="metric bad">collect_stone</span> is at just 1%. This may indicate a Craftax environment quirk where the agent can place stone from initial inventory or spawn conditions. <span class="metric bad">place_furnace</span>, <span class="metric bad">collect_coal</span>, <span class="metric bad">make_stone_pickaxe</span>, and <span class="metric bad">make_stone_sword</span> are all at 0%, confirming the agent has not learned the stone-tier crafting chain.</p>

    <p><strong>Tier 2 – Iron (avg 32.3%):</strong> <span class="metric good">make_iron_sword</span> at 97% is the most surprising result — this is a Tier 2 achievement appearing without any stone-tier prerequisites. This almost certainly reflects a mismatch between the achievement index mapping and the actual Craftax environment's achievement detection, or Craftax may have a shortcut path. Regardless of the cause, the agent is consistently triggering whatever game state this tracks. <span class="metric bad">make_iron_pickaxe</span> and <span class="metric bad">collect_diamond</span> remain at 0%.</p>

    <p><strong>Tiers 3–4 – Diamond & Combat (avg 0%):</strong> Complete zeros across all diamond-tier and combat-tier achievements. The agent has never crafted diamond tools, defeated a zombie, defeated a skeleton, or awakened the boss. These require deep tech-tree progression and combat skills that are beyond the agent's current behavioral repertoire at 500k steps.</p>
  </div>

  <div class="callout warning">
    <div class="callout-title">⚠ Anomalous Achievement Mapping</div>
    <p>Two achievements display patterns inconsistent with the expected prerequisite chain: <strong>place_stone</strong> (100%) without collect_stone (1%), and <strong>make_iron_sword</strong> (97%) without any stone-tier tools. This may indicate: (a) a mismatch between CRAFTAX_ACHIEVEMENT_NAMES ordering and the environment's actual achievement indices, (b) Craftax-specific mechanics (e.g., starting inventory, environmental spawns), or (c) an indexing bug in the achievement tracking. It's recommended to verify the achievement-to-index mapping against the Craftax source code.</p>
  </div>
</div>


<!-- ═══════════ SECTION 5: Tier Curves ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">05</span>
    <h2>Per-Tier Learning Dynamics</h2>
  </div>
  <p class="description">Individual achievement learning curves grouped by tier, revealing the temporal ordering of skill acquisition.</p>

  <div class="figure-block">
    <img src="fig5_tier_curves.png" alt="Tier Learning Curves">
    <div class="figure-caption">Figure 5: Achievement rates over training for each tier. Each line is one achievement within that tier, smoothed with EMA-100.</div>
  </div>

  <div class="analysis">
    <p><strong>Tier 0 – Basic</strong> shows clear stratification: <span class="metric good">collect_sapling</span> and <span class="metric good">collect_wood</span> are learned almost immediately (within the first 10k steps), consistent with these being spatially proximate resources the agent encounters by random exploration. <span class="metric">collect_drink</span> ramps up more slowly, suggesting the agent needs to learn water-seeking behavior. <span class="metric warn">eat_cow</span> and <span class="metric warn">place_table</span> remain near-zero throughout — these require multi-step intentional behavior (chasing and killing a cow, gathering resources and placing a crafting table) that the agent's exploration hasn't rewarded sufficiently.</p>

    <p><strong>Tier 1 – Stone</strong> is dominated by the <span class="metric good">place_stone</span> anomaly (100% from very early). All other Tier 1 achievements flatline near zero, confirming the agent has not penetrated the stone-age crafting chain. The small blip in <span class="metric">collect_iron</span> (2%) and <span class="metric">collect_stone</span> (1%) suggests the agent occasionally stumbles upon these resources but hasn't learned to mine them deliberately.</p>

    <p><strong>Tier 2 – Iron</strong> shows the striking <span class="metric">make_iron_sword</span> curve ramping up alongside Tier 0 basics and saturating at 97%. The other Tier 2 achievements remain at zero. This isolated high-rate achievement strongly suggests an indexing artifact (see Section 4 callout).</p>

    <p><strong>Tiers 3 and 4</strong> are completely flat at zero, as expected. Diamond and combat achievements require deep tech-tree progression that the agent is nowhere close to reaching.</p>

    <p>The <strong>key takeaway from the tier dynamics</strong> is that the DreamerV3 world model has successfully learned the visual dynamics and basic resource interactions of Craftax, enabling reliable low-tier achievements, but has not developed the multi-step planning capability needed to execute crafting chains (table → pickaxe → stone → furnace → iron tools). This is a known challenge for model-based RL in open-ended environments — the imagination horizon of <span class="stat-highlight">15 steps</span> (from your config) may be insufficient to plan through 10+ step crafting sequences.</p>
  </div>
</div>


<!-- ═══════════ SECTION 6: Forgetting ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">06</span>
    <h2>Catastrophic Forgetting Analysis</h2>
  </div>
  <p class="description">Measuring whether the agent loses previously acquired skills during continued training — the central concern of Continual-Dreamer.</p>

  <div class="figure-block">
    <img src="fig6_forgetting.png" alt="Forgetting Analysis">
    <div class="figure-caption">Figure 6: Left — aggregate forgetting (mean across all achievements of peak − current rate) over training. Right — per-achievement forgetting at the final timestep.</div>
  </div>

  <div class="analysis">
    <p><strong>Aggregate Forgetting</strong> is defined as <span class="stat-highlight">F = (1/A) Σ max(0, peak_rate_a − current_rate_a)</span> averaged over all A=22 achievements. It measures how much of the agent's peak capability has been lost. A value of <span class="metric good">0.030</span> at the end of training is excellent — the agent has retained virtually all skills it ever learned.</p>

    <p>The temporal curve (left panel) shows forgetting spiking early in training (around 30k–80k steps) and then settling to near-zero. The early spike is expected: as the agent's exploration strategy shifts and the world model updates, some initially-discovered behaviors are temporarily lost before the policy stabilizes. The convergence to ~0.03 indicates that the DreamerV3 replay buffer (2M transitions, 50/50 recency/uniform sampling as per your config) is effectively preventing catastrophic forgetting in this single-task setting.</p>

    <p>The <strong>per-achievement forgetting</strong> (right panel) reveals that forgetting is concentrated in just a few achievements — primarily <span class="metric warn">place_table</span> and <span class="metric warn">eat_cow</span>, which peaked at slightly higher rates early in training and have since regressed. The dominant achievements (collect_sapling, place_stone, collect_wood, make_iron_sword) show zero forgetting, confirming they've been stably integrated into the agent's behavioral repertoire.</p>

    <p>This result is consistent with the Continual-Dreamer paper's finding that world models naturally resist forgetting thanks to the replay buffer. Your <span class="metric">50/50 recency-uniform sampling</span> strategy (from the config: <code>fracs: {priority: 0.0, recency: 0.5, uniform: 0.5}</code>) ensures old experiences remain in the training distribution, and the RSSM's latent dynamics effectively retain past knowledge. In a multi-task continual learning extension, reservoir sampling would be the recommended addition to maintain this low forgetting.</p>
  </div>

  <div class="callout success">
    <div class="callout-title">✓ Forgetting is well-controlled</div>
    <p>At 0.030 aggregate forgetting, this agent exhibits near-zero skill regression. The DreamerV3 replay mechanism with 50/50 recency-uniform sampling is working as intended. The Continual-Dreamer paper found that reservoir sampling on top of this baseline reduces forgetting further in multi-task settings — but for single-task, the current configuration is already excellent.</p>
  </div>
</div>


<!-- ═══════════ SECTION 7: Score Distribution ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">07</span>
    <h2>Score Distribution Evolution</h2>
  </div>
  <p class="description">How the distribution of episode returns shifts across five training phases.</p>

  <div class="figure-block">
    <img src="fig7_score_distribution.png" alt="Score Distribution">
    <div class="figure-caption">Figure 7: Normalized histograms of episode return for five equal training phases. Distribution shifts rightward and narrows as training progresses.</div>
  </div>

  <div class="analysis">
    <p>This histogram decomposition reveals how the agent's behavior distribution evolves. In the <strong>first phase (0–100k)</strong>, the distribution is centered near 0–2.0 with a heavy left tail — many episodes end with minimal or zero return. By the <strong>middle phases (200k–400k)</strong>, the distribution has shifted dramatically rightward to a 3–4 peak with much less density below 2. In the <strong>final phase (400k–500k)</strong>, the distribution has tightened around <span class="stat-highlight">3.1–4.1</span> with a standard deviation of <span class="metric">0.86</span>.</p>

    <p>The <strong>narrowing variance</strong> is particularly informative: the agent's policy has become more deterministic and consistent. Early in training, episode outcomes are highly stochastic because the policy is exploring randomly. By the end, the agent executes a fairly consistent "recipe" — collect sapling, collect wood, place stone, collect drink, and a few more — with variation coming primarily from map layout rather than policy randomness.</p>

    <p>The <strong>right tail extending to 7.1</strong> in later phases represents exceptional episodes where the agent unlocks 6–7 achievements in a single life. These high-return episodes are rare but important — they represent the frontier of the agent's capability and the kinds of trajectories that drive further learning through the replay buffer's priority sampling.</p>

    <p>The <strong>absence of a long left tail</strong> in later phases confirms the success rate metric: the agent almost never has catastrophically bad episodes anymore. The minimum return in the final 100 episodes stays above 1.0.</p>
  </div>
</div>


<!-- ═══════════ SECTION 8: Stats Dashboard ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">08</span>
    <h2>Training Statistics Dashboard</h2>
  </div>
  <p class="description">Six complementary metrics providing a holistic view of training dynamics.</p>

  <div class="figure-block">
    <img src="fig8_stats_dashboard.png" alt="Stats Dashboard">
    <div class="figure-caption">Figure 8: (Top L) Return percentiles with IQR band. (Top C) Episode length EMA. (Top R) Episode throughput. (Bottom L) Running max return. (Bottom C) Achievements per episode. (Bottom R) Cumulative achievement discovery.</div>
  </div>

  <div class="analysis">
    <p><strong>Return Percentiles</strong> (top-left): The 25th–75th percentile band (shaded) narrows from a width of ~2.5 early in training to ~1.2 by the end, while the median climbs from 0.5 to 4.0. The tight interquartile range confirms consistent policy performance. The 25th percentile crossing 2.5 by 300k steps means even the "worst" quarter of episodes still achieves meaningful progress.</p>

    <p><strong>Episode Length</strong> (top-center): Monotonic increase from ~180 to ~320 steps, with the steepest climb between 50k–200k. This plateau around 320 may indicate the agent has reached a natural equilibrium between survival capability and risk-taking behavior. In Craftax, longer survival eventually plateaus because the environment gets progressively harder as enemies spawn and resources deplete.</p>

    <p><strong>Episode Throughput</strong> (top-right): Approximately 35–40 episodes per 10k environment steps, which translates to an average episode length of ~250–285 steps. The slight decrease in throughput over time is consistent with longer episodes — fewer episodes complete per unit of environment interaction. This is a healthy signal: the agent is trading episode count for episode quality.</p>

    <p><strong>Running Max Return</strong> (bottom-left): The best-ever episode return reaches <span class="metric good">7.1</span>, first achieved around 130k steps and matched again later. This represents 7 unique achievements in a single life — an impressive feat showing the agent is capable of deep tech-tree exploration even if the average episode only achieves 3–4. The fact that the running max hasn't increased beyond 7.1 since ~130k steps suggests the agent may need more training time or architectural changes (e.g., longer imagination horizon) to push further.</p>

    <p><strong>Achievements per Episode</strong> (bottom-center): Tracks the raw count of unique achievements per episode. This closely mirrors the return curve but is integer-valued. The EMA settles at approximately 3.5 achievements per episode, with occasional spikes to 5–7.</p>

    <p><strong>Cumulative Achievement Discovery</strong> (bottom-right): Of the 22 possible achievements, the agent discovers <span class="metric">9</span> at a >5% success rate. Nearly all discoveries happen within the first 110k steps, after which the agent explores no new achievements. This "discovery plateau" is a key diagnostic: it tells us the agent's exploration has saturated — the world model's prediction uncertainty (which drives exploration in DreamerV3 via the ensemble disagreement mechanism) is no longer finding novel state-action regions. In a continual learning extension, this is precisely where task switching or curriculum learning would be introduced to push the agent into new regions of the state space.</p>
  </div>
</div>


<!-- ═══════════ SECTION 9: Configuration Review ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">09</span>
    <h2>Configuration & Hyperparameters</h2>
  </div>
  <p class="description">Key hyperparameters from your DreamerV3 config that directly impact the observed performance.</p>

  <div class="config-grid">
    <div class="config-item"><span class="key">RSSM Stoch</span><span class="val">32 × 24 classes</span></div>
    <div class="config-item"><span class="key">RSSM Deter</span><span class="val">3072</span></div>
    <div class="config-item"><span class="key">Imagination Horizon</span><span class="val">15 steps</span></div>
    <div class="config-item"><span class="key">Action Repeat Horizon</span><span class="val">333</span></div>
    <div class="config-item"><span class="key">Learning Rate</span><span class="val">4e-5</span></div>
    <div class="config-item"><span class="key">Batch Size</span><span class="val">16 × 64</span></div>
    <div class="config-item"><span class="key">Replay Size</span><span class="val">2M transitions</span></div>
    <div class="config-item"><span class="key">Replay Sampling</span><span class="val">50% recency / 50% uniform</span></div>
    <div class="config-item"><span class="key">Train Ratio</span><span class="val">64.0</span></div>
    <div class="config-item"><span class="key">Compute Dtype</span><span class="val">bfloat16</span></div>
    <div class="config-item"><span class="key">Policy Distribution</span><span class="val">categorical (discrete)</span></div>
    <div class="config-item"><span class="key">Reward Head</span><span class="val">symexp twohot (255 bins)</span></div>
    <div class="config-item"><span class="key">Entropy Coefficient</span><span class="val">3e-4</span></div>
    <div class="config-item"><span class="key">Lambda (GAE)</span><span class="val">0.95</span></div>
    <div class="config-item"><span class="key">Env Instances</span><span class="val">16 parallel</span></div>
    <div class="config-item"><span class="key">Total Steps</span><span class="val">500k</span></div>
  </div>

  <div class="analysis">
    <p>Your configuration follows the standard DreamerV3 "S" (small) profile with <span class="stat-highlight">384-unit</span> hidden layers and a <span class="stat-highlight">32×24</span> categorical RSSM latent (768 effective stochastic dimensions + 3072 deterministic). The <span class="metric">15-step imagination horizon</span> is a key bottleneck — Craftax's multi-step crafting recipes (e.g., collect wood → place table → make pickaxe → collect stone → ...) require 8–15 real actions per chain, meaning the imagination can only plan through one or two crafting operations before the rollout terminates.</p>

    <p>The <span class="metric">50/50 recency-uniform replay sampling</span> directly mirrors the Continual-Dreamer paper's recommended strategy for balancing plasticity and stability. Combined with the <span class="stat-highlight">2M replay buffer</span>, this ensures old episodes (contributing stability) and recent episodes (contributing plasticity) are equally represented in each training batch — exactly the setup that achieved the best performance in the paper's Minihack experiments.</p>

    <p>The <span class="metric">train ratio of 64</span> means for every environment step, the agent performs 64 gradient updates — a very high ratio that prioritizes sample efficiency over wall-clock time. This is appropriate for Craftax where environment steps are cheap (JAX-vectorized) but model quality is the bottleneck.</p>

    <p>The <span class="metric">entropy coefficient of 3e-4</span> is relatively low, which may contribute to the exploration plateau observed in Section 8. A higher entropy coefficient or explicit exploration bonuses (à la Plan2Explore from the Continual-Dreamer paper) could help the agent discover the missing 13 achievements.</p>
  </div>
</div>


<!-- ═══════════ SECTION 10: Summary & Recommendations ═══════════ -->
<div class="section">
  <div class="section-header">
    <span class="number">10</span>
    <h2>Summary & Recommendations</h2>
  </div>

  <div class="callout success">
    <div class="callout-title">✓ What's Working</div>
    <p><strong>World model learning is effective:</strong> The RSSM has learned Craftax's visual dynamics well enough to support policy improvement to 3.78 mean return. <strong>Forgetting is minimal:</strong> At 0.030, the replay buffer strategy prevents catastrophic skill loss. <strong>Stable convergence:</strong> The score distribution tightens with no training instabilities. <strong>Good sample efficiency:</strong> 1,768 episodes at 500k steps with a train ratio of 64 demonstrates efficient use of data.</p>
  </div>

  <div class="callout critical">
    <div class="callout-title">✗ Key Limitations</div>
    <p><strong>Exploration has plateaued:</strong> Only 9/22 achievements discovered, all within the first 110k steps. The agent cannot plan through multi-step crafting chains. <strong>Core crafting chain broken:</strong> place_table at 9% bottlenecks all tool crafting → pickaxe → mining → furnace → advanced tools. <strong>Zero combat capability:</strong> Tiers 3–4 are untouched, requiring architectural or algorithmic changes to reach.</p>
  </div>

  <div class="callout info">
    <div class="callout-title">→ Recommendations for Next Steps</div>
    <p><strong>1. Increase imagination horizon</strong> from 15 to 30–50 steps. Craftax crafting chains require 10+ steps, and the current horizon may be too short for the policy to learn multi-step plans inside the world model's imagination.</p>
    <p><strong>2. Add Plan2Explore intrinsic reward</strong> as in Continual-Dreamer. The current agent uses no explicit exploration bonus. An ensemble disagreement signal would drive the policy toward unexplored state-action regions, potentially breaking through the crafting chain bottleneck.</p>
    <p><strong>3. Extend training to 1–2M steps.</strong> The return curve has not fully plateaued — the slope from 300k–500k is still positive (~0.15 return per 100k steps). More training time may yield incremental improvements even without architectural changes.</p>
    <p><strong>4. Verify achievement index mapping</strong> against the Craftax source. The place_stone (100%) and make_iron_sword (97%) anomalies suggest a possible mismatch that could affect reward signal interpretation.</p>
    <p><strong>5. For continual learning extension:</strong> Add reservoir sampling to the replay buffer (as in Continual-Dreamer) and implement TD-error based curriculum learning to replace the current uniform task sampling. The low forgetting in this single-task baseline is a strong foundation.</p>
  </div>
</div>

</div>

<footer>
  DreamerV3 × Craftax Performance Analysis · Generated February 2026 · Based on Continual-Dreamer (Kessler et al., CoLLAs 2023)
</footer>

</body>
</html>